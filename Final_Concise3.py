import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')

# Set random seeds - å®Œå…¨å¤åˆ¶myDefines2çš„è®¾ç½®
torch.manual_seed(42)
np.random.seed(42)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Import custom modules
from myDefines3 import preprocess_data, EnhancedUnifiedDataset, EnhancedGenerator, EnhancedDiscriminator, train_gan, predict_unified, calculate_metrics

# ============================================
# æ›´æ–°å‚æ•°è®¾ç½®ä»¥åŒ¹é…myDefines2çš„æ”¹è¿›
# ============================================
PARAMS = {
    'n_heads': 2,
    'WINDOW_SIZE': 50,
    'macro_weight_target': 0.3,
    'LAMBDA_SPECTRAL': 0.0,
    # Fixed parameters
    'BATCH_SIZE': 16,
    'NUM_EPOCHS': 30,
    'HIDDEN_DIM': 96,
    'N_LAYERS': 2,
    'DROPOUT': 0.3,
    'USE_RETURNS': True,
    'TEST_SPLIT': 0.2,
    'LAMBDA_REG': 0.5,
    'LAMBDA_VARIANCE': 10,
    'LAMBDA_DIRECTIONAL': 0.5
}

# ============================================
# æ›´æ–°å®éªŒå‡½æ•°ä»¥åŒ¹é…myDefines2çš„æ”¹è¿›
# ============================================
def Run_Experiment(PARAMS, myDataset="SP500"):
    """å®Œå…¨åŒ¹é…myDefines2é€»è¾‘çš„å®éªŒå‡½æ•°"""
    print(f"\n{'='*60}")
    print(f"è¿è¡Œå•æ¬¡å®éªŒ - ä½¿ç”¨å¢å¼ºç‰ˆæ¨¡å‹æ¶æ„")
    print(f"æ•°æ®é›†: {myDataset}")
    print(f"{'='*60}")
    
    try:
        # Data preprocessing - ä½¿ç”¨myDefines2çš„æ”¹è¿›ç‰ˆé¢„å¤„ç†
        print("ğŸ”§ æ•°æ®é¢„å¤„ç†ä¸­...")
        data = preprocess_data(
            f"Datasets/merged{myDataset}.csv",
            window_size=PARAMS['WINDOW_SIZE'],
            use_returns=PARAMS['USE_RETURNS'],
            test_split=PARAMS['TEST_SPLIT'],
            dataset=myDataset
        )
        
        (X_stock_train, X_macro_train, y_train, prev_train, freq_train, cyc_train,
         X_stock_test, X_macro_test, y_test, prev_test, freq_test, cyc_test,
         scalers, df, date_correspondences, base_prices_train, base_prices_test) = data
        
        print(f"ğŸ“Š æ•°æ®åŠ è½½å®Œæˆ:")
        print(f"  è®­ç»ƒæ ·æœ¬: {len(X_stock_train)}")
        print(f"  æµ‹è¯•æ ·æœ¬: {len(X_stock_test)}")
        print(f"  è‚¡ç¥¨ç‰¹å¾ç»´åº¦: {X_stock_train.shape[2]}")
        print(f"  å®è§‚ç‰¹å¾ç»´åº¦: {X_macro_train.shape[1] * X_macro_train.shape[2]}")
        print(f"  é¢‘åŸŸç‰¹å¾ç»´åº¦: {freq_train.shape[1]}")
        print(f"  å‘¨æœŸç‰¹å¾ç»´åº¦: {cyc_train.shape[1]}")
        
        # Create dataset and dataloader - ä½¿ç”¨å¢å¼ºç‰ˆæ•°æ®é›†
        train_dataset = EnhancedUnifiedDataset(
            X_stock_train, X_macro_train, y_train, freq_train, cyc_train, prev_train
        )
        train_loader = DataLoader(
            train_dataset, 
            batch_size=PARAMS['BATCH_SIZE'], 
            shuffle=True,
            drop_last=True  # ç¡®ä¿æ‰¹æ¬¡å¤§å°ä¸€è‡´
        )
        
        # Model dimensions - è®¡ç®—æ­£ç¡®çš„ç»´åº¦
        dims = {
            'stock_input_dim': X_stock_train.shape[2],
            'macro_input_dim': X_macro_train.shape[1] * X_macro_train.shape[2],
            'freq_dim': freq_train.shape[1],
            'cyc_dim': cyc_train.shape[1],
            'output_dim': 1
        }
        
        print(f"ğŸ—ï¸ æ¨¡å‹ç»´åº¦:")
        for key, value in dims.items():
            print(f"  {key}: {value}")
        
        # Initialize models - ä½¿ç”¨å¢å¼ºç‰ˆæ¨¡å‹æ¶æ„
        print("ğŸ¤– åˆå§‹åŒ–ç”Ÿæˆå™¨...")
        generator = EnhancedGenerator(
            stock_input_dim=dims['stock_input_dim'], 
            macro_input_dim=dims['macro_input_dim'], 
            freq_dim=dims['freq_dim'], 
            cyclical_dim=dims['cyc_dim'],
            hidden_dim=PARAMS['HIDDEN_DIM'], 
            output_dim=dims['output_dim'],
            n_heads=PARAMS['n_heads'], 
            n_layers=PARAMS['N_LAYERS'], 
            dropout=PARAMS['DROPOUT']
        ).to(device)
        
        print("ğŸ›¡ï¸ åˆå§‹åŒ–åˆ¤åˆ«å™¨...")
        discriminator = EnhancedDiscriminator(
            stock_input_dim=dims['stock_input_dim'],
            macro_input_dim=dims['macro_input_dim'], 
            freq_dim=dims['freq_dim'], 
            cyclical_dim=dims['cyc_dim'],
            hidden_dim=PARAMS['HIDDEN_DIM'], 
            n_heads=PARAMS['n_heads'], 
            n_layers=PARAMS['N_LAYERS'], 
            dropout=PARAMS['DROPOUT']
        ).to(device)
        
        # æƒé‡è°ƒæ•´é€»è¾‘ - æ”¹è¿›ç‰ˆ
        param_name = 'macro_weight_target'  # æ˜ç¡®æŒ‡å®šå½“å‰è°ƒä¼˜å‚æ•°
        
        print(f"ğŸ”§ æƒé‡è°ƒæ•´é€»è¾‘:")
        print(f"  å½“å‰å‚æ•°: {param_name}")
        print(f"  ç›®æ ‡å®è§‚æƒé‡: {PARAMS['macro_weight_target']}")
        print(f"  åŸºçº¿å®è§‚æƒé‡: {PARAMS['macro_weight_target']}")
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦è°ƒæ•´æƒé‡
        if param_name == 'macro_weight_target' or PARAMS['macro_weight_target'] != PARAMS['macro_weight_target']:
            print("âœ… æ‰§è¡Œæƒé‡è°ƒæ•´")
            with torch.no_grad():
                new_weights = torch.tensor([
                    2.0,  # stock - ä¸»è¦æƒé‡
                    PARAMS['macro_weight_target'],  # macro - å¯è°ƒæƒé‡
                    0.2,  # freq - å›ºå®šæƒé‡
                    0.1   # cyclical - å›ºå®šæƒé‡
                ]).to(device)
                generator.fusion_weights.data = new_weights
            print(f"  è°ƒæ•´åæƒé‡: {generator.fusion_weights.data}")
        else:
            print("âŒ ä½¿ç”¨é»˜è®¤æƒé‡")
            print(f"  é»˜è®¤æƒé‡: {generator.fusion_weights.data}")
        
        # æ¨¡å‹å‚æ•°ç»Ÿè®¡
        total_PARAMS = sum(p.numel() for p in generator.parameters())
        trainable_PARAMS = sum(p.numel() for p in generator.parameters() if p.requires_grad)
        print(f"ğŸ“Š ç”Ÿæˆå™¨å‚æ•°: æ€»è®¡={total_PARAMS:,}, å¯è®­ç»ƒ={trainable_PARAMS:,}")
        
        total_PARAMS_d = sum(p.numel() for p in discriminator.parameters())
        trainable_PARAMS_d = sum(p.numel() for p in discriminator.parameters() if p.requires_grad)
        print(f"ğŸ“Š åˆ¤åˆ«å™¨å‚æ•°: æ€»è®¡={total_PARAMS_d:,}, å¯è®­ç»ƒ={trainable_PARAMS_d:,}")
        
        # Training - ä½¿ç”¨å¢å¼ºç‰ˆè®­ç»ƒå‡½æ•°
        print("ğŸš€ å¼€å§‹è®­ç»ƒ...")

        time_start = time.time()  # è®°å½•å¼€å§‹æ—¶é—´

        losses, fusion_weights_history = train_gan(
            generator=generator, 
            discriminator=discriminator, 
            dataloader=train_loader, 
            num_epochs=PARAMS['NUM_EPOCHS'],
            g_lr=0.0001,  # ä½¿ç”¨myDefines2çš„é»˜è®¤å­¦ä¹ ç‡
            d_lr=0.0001,
            lambda_reg=PARAMS['LAMBDA_REG'], 
            lambda_variance=PARAMS['LAMBDA_VARIANCE'],
            lambda_spectral=PARAMS['LAMBDA_SPECTRAL'], 
            lambda_directional=PARAMS['LAMBDA_DIRECTIONAL'],
            clip_value=1.0,  # æ¢¯åº¦è£å‰ª
            use_directional_loss=PARAMS['USE_RETURNS']
        )
        
        time_end = time.time()  # è®°å½•ç»“æŸæ—¶é—´
        elapsed_time = time_end - time_start
        print(f"â±ï¸ è®­ç»ƒæ—¶é—´: {elapsed_time:.2f} ç§’")


        print("âœ… è®­ç»ƒå®Œæˆï¼")
        
        # æ˜¾ç¤ºè®­ç»ƒè¿‡ç¨‹ä¸­çš„æƒé‡å˜åŒ–
        if fusion_weights_history:
            final_weights = fusion_weights_history[-1]
            print(f"ğŸ¯ æœ€ç»ˆèåˆæƒé‡: è‚¡ç¥¨={final_weights[0]:.3f}, å®è§‚={final_weights[1]:.3f}, "
                  f"é¢‘åŸŸ={final_weights[2]:.3f}, å‘¨æœŸ={final_weights[3]:.3f}")
        
        # Evaluation - ä½¿ç”¨æ”¹è¿›çš„è¯„ä¼°æ–¹æ³•
        print("ğŸ“ˆ æ¨¡å‹è¯„ä¼°ä¸­...")
        generator.eval()
        
        predictions = []
        predicted_prices = []
        actual_prices = []
        actual_returns = []
        prev_values_for_direction = []
        
        # è·å–å®é™…ä»·æ ¼å’Œæ”¶ç›Šç‡
        test_start_idx = int(len(date_correspondences) * (1 - PARAMS['TEST_SPLIT']))
        
        for i in range(len(X_stock_test)):
            if test_start_idx + i < len(date_correspondences):
                actual_prices.append(date_correspondences[test_start_idx + i]['target_close_price'])
        
        # é¢„æµ‹
        with torch.no_grad():
            time_start = time.time()  # è®°å½•é¢„æµ‹å¼€å§‹æ—¶é—´
            for i in range(len(X_stock_test)):
                if PARAMS['USE_RETURNS']:

                    pred_price, fusion_weights, pred_return = predict_unified(
                        generator, X_stock_test[i], X_macro_test[i], freq_test[i], cyc_test[i],
                        scalers, last_price=base_prices_test[i], use_returns=PARAMS['USE_RETURNS']
                    )
                    predictions.append(pred_return)
                    predicted_prices.append(pred_price)
                    prev_values_for_direction.append(base_prices_test[i])
                else:
                    pred_price, fusion_weights, _ = predict_unified(
                        generator, X_stock_test[i], X_macro_test[i], freq_test[i], cyc_test[i],
                        scalers, use_returns=PARAMS['USE_RETURNS']
                    )
                    predictions.append(pred_price)
                    predicted_prices.append(pred_price)
                    if i > 0:
                        prev_values_for_direction.append(predicted_prices[i-1])
        # è½¬æ¢ä¸ºæ•°ç»„
        predictions = np.array(predictions)
        predicted_prices = np.array(predicted_prices)
        actual_prices = np.array(actual_prices[:len(predicted_prices)])
        prev_values_for_direction = np.array(prev_values_for_direction)
        


        # è·å–å®é™…æ”¶ç›Šç‡
        if PARAMS['USE_RETURNS']:
            if scalers['target_scaler']:
                actual_returns = scalers['target_scaler'].inverse_transform(y_test)
            else:
                actual_returns = y_test
            actual_returns = actual_returns.flatten()[:len(predictions)]
        else:
            actual_returns = y_test.flatten()[:len(predictions)]
        
        time_end = time.time()  # è®°å½•é¢„æµ‹ç»“æŸæ—¶é—´
        elapsed_time = time_end - time_start
        print(f"â±ï¸ é¢„æµ‹æ—¶é—´: {elapsed_time:.2f} ç§’")

        # è®¡ç®—æŒ‡æ ‡ - ä½¿ç”¨æ”¹è¿›çš„è®¡ç®—æ–¹æ³•
        print("ğŸ“Š è®¡ç®—æ€§èƒ½æŒ‡æ ‡...")
        
        # ä»·æ ¼æŒ‡æ ‡ï¼ˆä½¿ç”¨å‰ä¸€å¤©ä»·æ ¼ä½œä¸ºå‚è€ƒï¼‰
        price_metrics = calculate_metrics(
            predicted_prices, actual_prices, 
            prev_values=prev_values_for_direction if len(prev_values_for_direction) == len(predicted_prices) else None,
            prefix="Price"
        )
        
        # æ”¶ç›Šç‡æŒ‡æ ‡
        if PARAMS['USE_RETURNS']:
            return_metrics = calculate_metrics(predictions, actual_returns, prefix="Return")
        else:
            return_metrics = {}
        
        # æ•´ç†ç»“æœ
        results = {
            'MAE': price_metrics['Price MAE'],
            'RMSE': price_metrics['Price RMSE'],
            'MAPE': price_metrics['Price MAPE'] if price_metrics['Price MAPE'] is not None else 0,
            'Direction_Accuracy': return_metrics.get('Return Direction Accuracy', 
                                                   price_metrics.get('Price Direction Accuracy', 0)),
            'Correlation': price_metrics.get('Price Correlation', 0)
        }
        
        print(f"âœ… è¯„ä¼°å®Œæˆ!")
        print(f"  ä»·æ ¼MAE: ${results['MAE']:.2f}")
        print(f"  ä»·æ ¼RMSE: ${results['RMSE']:.2f}")
        print(f"  ä»·æ ¼MAPE: {results['MAPE']:.2f}%")
        print(f"  æ–¹å‘å‡†ç¡®ç‡: {results['Direction_Accuracy']:.2f}%")
        print(f"  ç›¸å…³ç³»æ•°: {results['Correlation']:.4f}")
        
        return results, price_metrics, return_metrics
        
    except Exception as e:
        print(f"âŒ å®éªŒå¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return None, None, None



# ============================================
# Main
# ============================================
if __name__ == "__main__":

    import time
    start_time = time.time()  # Start timer
    print(f"ğŸš€ å¢å¼ºç‰ˆè‚¡ç¥¨é¢„æµ‹æ¨¡å‹å®éªŒ")
    print(f"ğŸ”§ è®¾å¤‡: {device}")
    print(f"ğŸ“‹ å®éªŒå‚æ•°:")
    for key, value in PARAMS.items():
        print(f"  {key}: {value}")
    
    # è¿è¡Œå®éªŒ
    results, price_metrics, return_metrics = Run_Experiment(PARAMS, "SP500")
    
    if results:
        print("ğŸ’° ä»·æ ¼é¢„æµ‹æ€§èƒ½:")
        for name, value in price_metrics.items():
            if value is not None:
                if 'MAE' in name or 'RMSE' in name:
                    print(f"  {name}: ${value:.2f}")
                elif 'MAPE' in name or 'Accuracy' in name:
                    print(f"  {name}: {value:.2f}%")
                else:
                    print(f"  {name}: {value:.4f}")
        
        if PARAMS['USE_RETURNS'] and return_metrics:
            print(f"\nğŸ“Š æ”¶ç›Šç‡é¢„æµ‹æ€§èƒ½:")
            for name, value in return_metrics.items():
                if value is not None:
                    if 'Direction Accuracy' in name or 'MAPE' in name:
                        print(f"  {name}: {value:.2f}%")
                    else:
                        print(f"  {name}: {value:.4f}")
        
        print(f"\nğŸ¯ æ ¸å¿ƒæŒ‡æ ‡æ€»ç»“:")
        print(f"  ğŸ“ˆ æ–¹å‘å‡†ç¡®ç‡: {results['Direction_Accuracy']:.2f}%")
        print(f"  ğŸ’² ä»·æ ¼MAE: ${results['MAE']:.2f}")
        print(f"  ğŸ“ ä»·æ ¼RMSE: ${results['RMSE']:.2f}")
        print(f"  ğŸ“Š ä»·æ ¼MAPE: {results['MAPE']:.2f}%")
        print(f"  ğŸ”— ç›¸å…³ç³»æ•°: {results['Correlation']:.4f}")
        
    else:
        print("âŒ å®éªŒå¤±è´¥ï¼Œè¯·æ£€æŸ¥æ•°æ®æ–‡ä»¶å’Œå‚æ•°è®¾ç½®ã€‚")


    end_time = time.time()  # End timer
    elapsed_time = end_time - start_time
    print(f"\nTotal execution time: {elapsed_time:.2f} seconds")


